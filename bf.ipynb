{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea0dad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 13 個 xlsx 檔案在 file 1-4 中\n",
      "\n",
      "處理: E:\\EarthScienceFair_Data\\1\\G2\\1-G2-1.xlsx\n",
      "  組別: 1-G2, 質量: 1.9505 kg\n",
      "  ye 主頻率: 2.66 Hz\n",
      "  yd 主頻率: 2.67 Hz\n",
      "  yc 主頻率: 2.67 Hz\n",
      "  yb 主頻率: 2.75 Hz\n",
      "  ya 主頻率: 2.68 Hz\n",
      "  系統自然頻率: 2.69 Hz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1596 and the array at index 1 has size 1597",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 326\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  讀取 zip 錯誤: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# 畫畫!\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m pca = PCA(n_components=\u001b[32m2\u001b[39m)\n\u001b[32m    328\u001b[39m pca.fit(np.column_stack((x,y)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\miyun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_shape_base_impl.py:669\u001b[39m, in \u001b[36mcolumn_stack\u001b[39m\u001b[34m(tup)\u001b[39m\n\u001b[32m    667\u001b[39m         arr = array(arr, copy=\u001b[38;5;28;01mNone\u001b[39;00m, subok=\u001b[38;5;28;01mTrue\u001b[39;00m, ndmin=\u001b[32m2\u001b[39m).T\n\u001b[32m    668\u001b[39m     arrays.append(arr)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1596 and the array at index 1 has size 1597"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import medfilt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings for file structure\n",
    "base_path = r\"E:\\EarthScienceFair_Data\"\n",
    "target_folders = [\"1\", \"2\", \"3\", \"4\"]  # 只處理 file 1-4\n",
    "\n",
    "\n",
    "# Mass configuration (kg) - 用 dict 對應不同組別的質量\n",
    "mass_dict = {\n",
    "    \"1-G2\": 1.9505,   # w1: 1950.5g\n",
    "    \"1-G3\": 2.0122,   # w2: 2012.2g\n",
    "    \"2-G1\": 2.2201,   # w3: 2220.1g\n",
    "    \"2-G2\": 2.6162,   # w4: 2616.2g\n",
    "    \"2-G3\": 2.6162,   # w4: 2616.2g\n",
    "    \"2-G4\": 2.6162,   # w4: 2616.2g\n",
    "    \"2-G5\": 1.9495,   # h3: 1949.5g\n",
    "    \"3-G1\": 1.9650,   # h1: 1965.0g\n",
    "    \"3-G2\": 1.9531,   # h2: 1953.1g\n",
    "    \"4-G1\": 1.8559,   # na: 1855.9g (對照組)\n",
    "}\n",
    "\n",
    "# 預設質量（如果某個組別不在 dict 中）\n",
    "default_mass = 1.9000\n",
    "\n",
    "def auto_trim_index(y, threshold_factor=3.0, window_size=20):\n",
    "    \"\"\"\n",
    "    偵測數據開始劇烈波動的索引位置\n",
    "    y: 數據數組\n",
    "    threshold_factor: 偵測門檻（噪訊標準差的幾倍），若切太多可以調高此數值\n",
    "    window_size: 滑動窗口大小\n",
    "    \"\"\"\n",
    "    if len(y) < window_size:\n",
    "        return 0\n",
    "    \n",
    "    # 以開頭一小段（前100點）計算基礎環境噪訊的標準差\n",
    "    base_std = np.std(y[:min(100, len(y))])\n",
    "    \n",
    "    # 轉換成 pandas Series 計算滑動標準差\n",
    "    y_series = pd.Series(y)\n",
    "    rolling_std = y_series.rolling(window=window_size).std().fillna(0).values\n",
    "    \n",
    "    # 找到第一個顯著超過噪訊門檻的地方\n",
    "    trigger_threshold = base_std * threshold_factor\n",
    "    indices = np.where(rolling_std > trigger_threshold)[0]\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        # 往前回溯一個窗口大小，確保包含完整的起跳波形\n",
    "        return max(0, indices[0] - window_size)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# 儲存所有實驗的原始數據，用於計算不確定度\n",
    "# 結構: {G1: {rms_x: [...], rms_y: [...], ...}, G2: {...}, ...}\n",
    "group_data = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "def filter_outliers(data, threshold=0.1):\n",
    "    \"\"\"\n",
    "    過濾異常值：若前一筆數據和後一筆數據落差超過 threshold，則標記為異常\n",
    "    回傳清理後的資料索引\n",
    "    \"\"\"\n",
    "    if len(data) <= 1:\n",
    "        return np.ones(len(data), dtype=bool)\n",
    "    \n",
    "    valid_mask = np.ones(len(data), dtype=bool)\n",
    "    \n",
    "    for i in range(1, len(data)):\n",
    "        if abs(data[i] - data[i-1]) > threshold:\n",
    "            valid_mask[i] = False\n",
    "    \n",
    "    return valid_mask\n",
    "\n",
    "def clean_side_data(side_data_raw, t_s_raw):\n",
    "    \"\"\"\n",
    "    清理側面數據：與 t_s 同步對齊，插值缺失值\n",
    "    \n",
    "    參數:\n",
    "        side_data_raw: 側面測量數據（ye, yd, yc, yb, ya）\n",
    "        t_s_raw: 原始時間序列\n",
    "    \"\"\"\n",
    "    # 找出 t_s 的有效索引\n",
    "    time_series = pd.Series(t_s_raw)\n",
    "    time_numeric = pd.to_numeric(time_series, errors='coerce')\n",
    "    valid_time_mask = time_numeric.notna()\n",
    "    \n",
    "    # 對側面數據也套用相同的遮罩\n",
    "    data_series = pd.Series(side_data_raw)\n",
    "    numeric_series = pd.to_numeric(data_series, errors='coerce')\n",
    "    \n",
    "    # 只保留 t_s 有效的那些行\n",
    "    aligned_series = numeric_series[valid_time_mask]\n",
    "    \n",
    "    # 插值處理（只在有效行範圍內）\n",
    "    if aligned_series.isna().any():\n",
    "        nan_count = aligned_series.isna().sum()\n",
    "        total_count = len(aligned_series)\n",
    "        print(f\"    發現 {nan_count}/{total_count} 個空白值，進行插值...\")\n",
    "        \n",
    "        aligned_series = aligned_series.interpolate(method='linear', limit_direction='both')\n",
    "        aligned_series = aligned_series.ffill().bfill()\n",
    "    \n",
    "    # 取出有效值\n",
    "    data_clean = aligned_series[aligned_series.notna()].values\n",
    "    \n",
    "    if len(data_clean) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    data_clean = data_clean.astype(float)\n",
    "    \n",
    "    # 過濾異常值 + 去中心化\n",
    "    valid_mask = filter_outliers(data_clean, threshold=0.1)\n",
    "    data_filtered = data_clean[valid_mask]\n",
    "    \n",
    "    if len(data_filtered) > 0:\n",
    "        data_filtered = data_filtered - np.mean(data_filtered)\n",
    "    \n",
    "    return data_filtered\n",
    "\n",
    "\n",
    "def clean_data_with_outlier_filter(data, time_data=False, allow_interpolation=True):\n",
    "    \"\"\"\n",
    "    清理數據：移除非數值、插值填補空白、轉換型別、並過濾異常值\n",
    "    （用於時間數據和俯瞰數據）\n",
    "    \"\"\"\n",
    "    data_series = pd.Series(data)\n",
    "    numeric_series = pd.to_numeric(data_series, errors='coerce')\n",
    "    \n",
    "    # 插值處理\n",
    "    if allow_interpolation and numeric_series.isna().any():\n",
    "        nan_count = numeric_series.isna().sum()\n",
    "        total_count = len(numeric_series)\n",
    "        print(f\"    發現 {nan_count}/{total_count} 個空白值，進行插值...\")\n",
    "        \n",
    "        numeric_series = numeric_series.interpolate(method='linear', limit_direction='both')\n",
    "        numeric_series = numeric_series.ffill().bfill()\n",
    "    \n",
    "    # 取出有效值\n",
    "    data_clean = numeric_series[numeric_series.notna()].values\n",
    "    \n",
    "    if len(data_clean) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    data_clean = data_clean.astype(float)\n",
    "    \n",
    "    if time_data:\n",
    "        # 時間數據：歸零\n",
    "        data_clean = data_clean - data_clean[0]\n",
    "        return data_clean\n",
    "    else:\n",
    "        # 測量數據：過濾異常值 + 去中心化\n",
    "        valid_mask = filter_outliers(data_clean, threshold=0.1)\n",
    "        data_filtered = data_clean[valid_mask]\n",
    "        \n",
    "        if len(data_filtered) > 0:\n",
    "            data_filtered = data_filtered - np.mean(data_filtered)\n",
    "        return data_filtered\n",
    "\n",
    "# Find all xlsx files in target folders only\n",
    "xlsx_files = []\n",
    "for folder in target_folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        xlsx_files.extend(glob.glob(os.path.join(folder_path, \"**\", \"*.xlsx\"), recursive=True))\n",
    "\n",
    "print(f\"找到 {len(xlsx_files)} 個 xlsx 檔案在 file 1-4 中\\n\")\n",
    "\n",
    "for tracker_file in xlsx_files:\n",
    "    print(\"處理:\", tracker_file)\n",
    "    \n",
    "    # 提取實驗組別\n",
    "    file_name = os.path.basename(tracker_file)\n",
    "    parts = file_name.split('-')\n",
    "    if len(parts) >= 2:\n",
    "        folder_num = parts[0]\n",
    "        group_name = parts[1]\n",
    "        combined_key = f\"{folder_num}-{group_name}\"\n",
    "    else:\n",
    "        print(\"檔名格式不符，跳過\")\n",
    "        continue\n",
    "    \n",
    "    mass = mass_dict.get(combined_key, default_mass)\n",
    "    print(f\"  組別: {combined_key}, 質量: {mass} kg\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(tracker_file)\n",
    "    except:\n",
    "        print(\"讀取失敗:\", tracker_file)\n",
    "        continue\n",
    "\n",
    "    # 讀取資料欄位\n",
    "    t_s_raw = df.iloc[:, 0].values\n",
    "    ye_raw = df.iloc[:, 1].values\n",
    "    yd_raw = df.iloc[:, 2].values\n",
    "    yc_raw = df.iloc[:, 3].values\n",
    "    yb_raw = df.iloc[:, 4].values\n",
    "    ya_raw = df.iloc[:, 5].values\n",
    "    \n",
    "    t_a_raw = df.iloc[:, -3].values\n",
    "    x_raw = df.iloc[:, -2].values\n",
    "    y_raw = df.iloc[:, -1].values\n",
    "\n",
    "    # === 1. 處理時間與基礎清理 ===\n",
    "    t_s = clean_data_with_outlier_filter(t_s_raw, time_data=True, allow_interpolation=False)\n",
    "    t_a = clean_data_with_outlier_filter(t_a_raw, time_data=True, allow_interpolation=False)\n",
    "    \n",
    "    # === 2. 處理俯瞰數據 (使用中位數濾波 + 自動裁切) ===\n",
    "    x_clean = clean_data_with_outlier_filter(x_raw, allow_interpolation=False)\n",
    "    y_clean = clean_data_with_outlier_filter(y_raw, allow_interpolation=False)\n",
    "\n",
    "    if len(y_clean) > 0:\n",
    "        # 修復 16s 尖峰雜訊\n",
    "        x_med = medfilt(x_clean, kernel_size=5)\n",
    "        y_med = medfilt(y_clean, kernel_size=5)\n",
    "        \n",
    "        # 自動偵測開始動的時間點點 (排除 0~3s)\n",
    "        start_idx = auto_trim_index(y_med)\n",
    "        x = x_med[start_idx:]\n",
    "        y = y_med[start_idx:]\n",
    "    else:\n",
    "        x, y = np.array([]), np.array([])\n",
    "\n",
    "    # 計算 RMS\n",
    "\n",
    "    if len(x) > 0 and len(y) > 0:\n",
    "        # 關鍵：計算 RMS 前必須先歸零 (去中心化)\n",
    "        x_centered = x - np.mean(x)\n",
    "        y_centered = y - np.mean(y)\n",
    "        \n",
    "        rms_x = np.sqrt(np.mean(x_centered**2))\n",
    "        rms_y = np.sqrt(np.mean(y_centered**2))\n",
    "        ratio = rms_y / rms_x if rms_x > 1e-8 else np.nan\n",
    "    else:\n",
    "        rms_x, rms_y, ratio = np.nan, np.nan, np.nan\n",
    "\n",
    "    # === 3. 處理側面數據 (與俯瞰數據同步裁切) ===\n",
    "    side_raw_data = {\"ye\": ye_raw, \"yd\": yd_raw, \"yc\": yc_raw, \"yb\": yb_raw, \"ya\": ya_raw}\n",
    "    layers = {}\n",
    "    \n",
    "    for key, raw_val in side_raw_data.items():\n",
    "        side_clean = clean_side_data(raw_val, t_s_raw)\n",
    "        if len(side_clean) > 0:\n",
    "            # 同樣經過濾波與同步裁切\n",
    "            side_med = medfilt(side_clean, kernel_size=5)\n",
    "            layers[key] = side_med[start_idx:] if len(side_med) > start_idx else side_med\n",
    "        else:\n",
    "            layers[key] = np.array([])\n",
    "\n",
    "    # FFT 頻率分析\n",
    "    main_freqs = []\n",
    "    for key, y_layer in layers.items():\n",
    "        if len(y_layer) < 10 or len(t_s) < 10:\n",
    "            continue\n",
    "        \n",
    "        # 1. 強制去中心化 (修正 RMS 異常)\n",
    "        y_centered = y_layer - np.mean(y_layer)\n",
    "        \n",
    "        # 2. 計算 FFT\n",
    "        dt = np.mean(np.diff(t_s[:len(y_centered)]))\n",
    "        if dt <= 0:\n",
    "            continue\n",
    "            \n",
    "        N = len(y_centered)\n",
    "        yf = fft(y_centered)\n",
    "        xf = fftfreq(N, dt)[:N//2]\n",
    "        amplitude = 2.0/N * np.abs(yf[:N//2])\n",
    "        \n",
    "        # 3. 設定合理的頻率遮罩 (1.2Hz ~ 5.0Hz)\n",
    "        # 避開 0.76Hz 的啟動/停止干擾\n",
    "        mask = (xf >= 1.2) & (xf <= 5.0)\n",
    "        \n",
    "        if np.any(mask):\n",
    "            masked_xf = xf[mask]\n",
    "            masked_amp = amplitude[mask]\n",
    "            \n",
    "            # 4. 使用加權平均找主頻率 (重心法)\n",
    "            # 即使頻譜是高原狀，算出來的數值也會非常穩定\n",
    "            # 公式: f_center = sum(f * amp) / sum(amp)\n",
    "            main_freq = np.sum(masked_xf * masked_amp) / np.sum(masked_amp)\n",
    "            \n",
    "            print(f\"  {key} 主頻率: {main_freq:.2f} Hz\")\n",
    "            main_freqs.append(main_freq)\n",
    "    \n",
    "    if len(main_freqs) > 0:\n",
    "        f_n = np.mean(main_freqs)\n",
    "        print(f\"  系統自然頻率: {f_n:.2f} Hz\")\n",
    "    else:\n",
    "        f_n = np.nan\n",
    "        print(\"  無法計算頻率\")\n",
    "\n",
    "    # 計算剛性\n",
    "    k = mass * (2 * np.pi * f_n)**2 if not np.isnan(f_n) else np.nan\n",
    "\n",
    "    # === 4. 讀取加速度資料 (保持原邏輯) ===\n",
    "    folder = os.path.dirname(tracker_file)\n",
    "    base_name = os.path.splitext(os.path.basename(tracker_file))[0]\n",
    "    all_zips = glob.glob(os.path.join(folder, \"*.zip\"))\n",
    "    zip_file = next((z for z in all_zips if os.path.splitext(os.path.basename(z))[0] == base_name), None)\n",
    "    \n",
    "    rms_acc = np.nan\n",
    "    if zip_file:\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "                csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
    "                if csv_files:\n",
    "                    with z.open(csv_files[0]) as f:\n",
    "                        acc_df = pd.read_csv(f)\n",
    "                        acc_abs_raw = acc_df.iloc[:, -1].values\n",
    "                        acc_series = pd.to_numeric(pd.Series(acc_abs_raw), errors='coerce').dropna().values\n",
    "                        if len(acc_series) > 0:\n",
    "                            # 加速度同樣做一次異常過濾\n",
    "                            acc_filtered = acc_series[filter_outliers(acc_series, threshold=0.1)]\n",
    "                            rms_acc = np.sqrt(np.mean(acc_filtered**2)) if len(acc_filtered) > 0 else np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"  讀取 zip 錯誤: {e}\")\n",
    "\n",
    "    # 畫畫!\n",
    "    data = np.column_stack((x, y))\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(np.column_stack((x,y)))\n",
    "    # 3. 繪製圖形\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # 繪製點 (Scatter)\n",
    "    plt.scatter(pca_data[:, 0], pca_data[:, 1], c=np.arange(len(x)), cmap='viridis', s=10)\n",
    "\n",
    "    # 繪製軌跡 (Plot) - 用線連起來\n",
    "    plt.plot(pca_data[:, 0], pca_data[:, 1], alpha=0.5, linestyle='-', color='gray')\n",
    "\n",
    "    # 加上標籤與美化\n",
    "    plt.title(\"PCA Trajectory Plot\")\n",
    "    plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)\")\n",
    "    plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)\")\n",
    "    plt.colorbar(label='Time / Sequence Index')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 4. 輸出\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    # 儲存結果與群組統計\n",
    "    results.append([tracker_file, combined_key, rms_x, rms_y, ratio, f_n, k, rms_acc])\n",
    "    \n",
    "    if combined_key not in group_data:\n",
    "        group_data[combined_key] = {p: [] for p in ['rms_x', 'rms_y', 'ratio', 'f_n', 'k', 'rms_acc']}\n",
    "    \n",
    "    for field, val in zip(['rms_x', 'rms_y', 'ratio', 'f_n', 'k', 'rms_acc'], \n",
    "                          [rms_x, rms_y, ratio, f_n, k, rms_acc]):\n",
    "        group_data[combined_key][field].append(val)\n",
    "    \n",
    "    print(f\"  已處理 {combined_key}\")\n",
    "\n",
    "# 建立結果 DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    \"File\", \"Group\", \"RMS_x\", \"RMS_y\", \"偏心比例\", \"主頻率(Hz)\", \"等效剛性(N/m)\", \"RMS加速度\"\n",
    "])\n",
    "\n",
    "# 計算每個群組的不確定度\n",
    "uncertainty_results = []\n",
    "\n",
    "for group_name, data in group_data.items():\n",
    "    print(f\"\\n計算 {group_name} 的不確定度:\")\n",
    "    \n",
    "    uncertainties = {}\n",
    "    \n",
    "    for param_name, values in data.items():\n",
    "        # 移除 NaN 值\n",
    "        valid_values = [v for v in values if not np.isnan(v)]\n",
    "        \n",
    "        if len(valid_values) >= 2:\n",
    "            # 計算平均值\n",
    "            mean_val = np.mean(valid_values)\n",
    "            \n",
    "            # 計算標準差 (樣本標準差，使用 n-1)\n",
    "            std_val = np.std(valid_values, ddof=1)\n",
    "            \n",
    "            # 計算標準不確定度 (u = s / sqrt(n))\n",
    "            n = len(valid_values)\n",
    "            u_val = std_val / np.sqrt(n)\n",
    "            \n",
    "            # 計算相對不確定度 (%)\n",
    "            relative_u = (u_val / mean_val * 100) if mean_val != 0 else np.nan\n",
    "            \n",
    "            uncertainties[param_name] = {\n",
    "                'mean': mean_val,\n",
    "                'std': std_val,\n",
    "                'u': u_val,\n",
    "                'relative_u': relative_u,\n",
    "                'n': n\n",
    "            }\n",
    "            \n",
    "            print(f\"  {param_name}:\")\n",
    "            print(f\"    平均值 = {mean_val:.6f}\")\n",
    "            print(f\"    標準差 = {std_val:.6f}\")\n",
    "            print(f\"    標準不確定度 u = {u_val:.6f}\")\n",
    "            print(f\"    相對不確定度 = {relative_u:.2f}%\")\n",
    "            print(f\"    樣本數 n = {n}\")\n",
    "        else:\n",
    "            uncertainties[param_name] = {\n",
    "                'mean': valid_values[0] if len(valid_values) == 1 else np.nan,\n",
    "                'std': np.nan,\n",
    "                'u': np.nan,\n",
    "                'relative_u': np.nan,\n",
    "                'n': len(valid_values)\n",
    "            }\n",
    "            print(f\"  {param_name}: 樣本數不足 (n={len(valid_values)})\")\n",
    "    \n",
    "    uncertainty_results.append({\n",
    "        'Group': group_name,\n",
    "        'RMS_x_mean': uncertainties['rms_x']['mean'],\n",
    "        'RMS_x_u': uncertainties['rms_x']['u'],\n",
    "        'RMS_y_mean': uncertainties['rms_y']['mean'],\n",
    "        'RMS_y_u': uncertainties['rms_y']['u'],\n",
    "        'ratio_mean': uncertainties['ratio']['mean'],\n",
    "        'ratio_u': uncertainties['ratio']['u'],\n",
    "        'f_n_mean': uncertainties['f_n']['mean'],\n",
    "        'f_n_u': uncertainties['f_n']['u'],\n",
    "        'k_mean': uncertainties['k']['mean'],\n",
    "        'k_u': uncertainties['k']['u'],\n",
    "        'rms_acc_mean': uncertainties['rms_acc']['mean'],\n",
    "        'rms_acc_u': uncertainties['rms_acc']['u'],\n",
    "        'sample_size': uncertainties['rms_x']['n']\n",
    "    })\n",
    "\n",
    "uncertainty_df = pd.DataFrame(uncertainty_results)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"File 1-4 分析結果:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"各組不確定度統計:\")\n",
    "print(\"=\"*80)\n",
    "print(uncertainty_df)\n",
    "\n",
    "# 存檔\n",
    "results_df.to_csv(\"analysis_results_file1to4.csv\", index=False, encoding='utf-8-sig')\n",
    "uncertainty_df.to_csv(\"uncertainty_file1to4.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n結果已儲存至:\")\n",
    "print(\"  - analysis_results_file1to4.csv (詳細結果)\")\n",
    "print(\"  - uncertainty_file1to4.csv (不確定度統計)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83067e63",
   "metadata": {},
   "source": [
    "Above code were provided by Claude and fixed by human."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

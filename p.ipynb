{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import medfilt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# 路徑與實驗設定\n",
    "# ============================================================\n",
    "base_path = r\"E:\\EarthScienceFair_Data\"\n",
    "target_folders = [\"5\"]  # 只處理 file 5\n",
    "\n",
    "# 各組質量 (kg)\n",
    "mass_dict = {\n",
    "    \"5-G1\": 7.0871,\n",
    "    \"5-G2\": 6.6211,\n",
    "    \"5-G3\": 6.1662,\n",
    "    \"5-G4\": 6.1870,\n",
    "    \"5-G5\": 6.2768,\n",
    "    \"5-G6\": 6.2560,\n",
    "    \"5-G7\": 6.2079,\n",
    "}\n",
    "default_mass = 6.2000\n",
    "\n",
    "group_data = {}\n",
    "results = []\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 工具函式\n",
    "# ============================================================\n",
    "\n",
    "def filter_outliers(data, threshold=0.1):\n",
    "    \"\"\"標記與前一點差距超過 threshold 的異常值，回傳 bool 遮罩\"\"\"\n",
    "    if len(data) <= 1:\n",
    "        return np.ones(len(data), dtype=bool)\n",
    "    valid_mask = np.ones(len(data), dtype=bool)\n",
    "    for i in range(1, len(data)):\n",
    "        if abs(data[i] - data[i-1]) > threshold:\n",
    "            valid_mask[i] = False\n",
    "    return valid_mask\n",
    "\n",
    "\n",
    "def clean_side_data(side_data_raw, t_s_raw):\n",
    "    \"\"\"\n",
    "    清理側面數據：\n",
    "      - 以 t_s 有效列為基準對齊\n",
    "      - 插值補空值\n",
    "      - 過濾異常值後去中心化\n",
    "    \"\"\"\n",
    "    time_series = pd.Series(t_s_raw)\n",
    "    time_numeric = pd.to_numeric(time_series, errors='coerce')\n",
    "    valid_time_mask = time_numeric.notna()\n",
    "\n",
    "    data_series = pd.Series(side_data_raw)\n",
    "    numeric_series = pd.to_numeric(data_series, errors='coerce')\n",
    "    aligned_series = numeric_series[valid_time_mask]\n",
    "\n",
    "    if aligned_series.isna().any():\n",
    "        nan_count = aligned_series.isna().sum()\n",
    "        total_count = len(aligned_series)\n",
    "        print(f\"    發現 {nan_count}/{total_count} 個空白值，進行插值...\")\n",
    "        aligned_series = aligned_series.interpolate(method='linear', limit_direction='both')\n",
    "        aligned_series = aligned_series.ffill().bfill()\n",
    "\n",
    "    data_clean = aligned_series[aligned_series.notna()].values\n",
    "    if len(data_clean) == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    data_clean = data_clean.astype(float)\n",
    "    valid_mask = filter_outliers(data_clean, threshold=0.1)\n",
    "    data_filtered = data_clean[valid_mask]\n",
    "\n",
    "    if len(data_filtered) > 0:\n",
    "        data_filtered = data_filtered - np.mean(data_filtered)\n",
    "\n",
    "    return data_filtered\n",
    "\n",
    "\n",
    "def clean_data_with_outlier_filter(data, time_data=False, allow_interpolation=True):\n",
    "    \"\"\"\n",
    "    通用清理函式：\n",
    "      - time_data=True  → 插值補齊後從 0 起算\n",
    "      - time_data=False → 過濾異常值 → 插值 → 去中心化\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(pd.Series(data), errors='coerce')\n",
    "\n",
    "    if time_data:\n",
    "        s = s.interpolate().ffill().bfill()\n",
    "        val = s.values\n",
    "        return val - val[0] if len(val) > 0 else np.array([])\n",
    "\n",
    "    valid_mask = filter_outliers(s.values, threshold=0.1)\n",
    "    s[~valid_mask] = np.nan\n",
    "\n",
    "    if allow_interpolation:\n",
    "        s = s.interpolate(method='linear', limit_direction='both').ffill().bfill()\n",
    "\n",
    "    val = s.values\n",
    "    if len(val) > 0:\n",
    "        val = val - np.nanmean(val)\n",
    "    return val\n",
    "\n",
    "\n",
    "def auto_trim_index(y_signal, skip_seconds=3, dt=None, fps=30):\n",
    "    \"\"\"\n",
    "    自動偵測訊號開始移動的時間點（排除前幾秒靜止段）\n",
    "    若無法判斷則直接回傳 0\n",
    "    \"\"\"\n",
    "    skip_frames = int(skip_seconds * fps) if dt is None else int(skip_seconds / dt)\n",
    "    skip_frames = min(skip_frames, len(y_signal) - 1)\n",
    "    return skip_frames\n",
    "\n",
    "\n",
    "def process_overhead_pair(x_raw, y_raw, t_a, mass, label, combined_key):\n",
    "    \"\"\"\n",
    "    處理一組俯瞰數據 (x, y)：\n",
    "      - 中位數濾波 + 自動裁切\n",
    "      - PCA 分析（繪圖）\n",
    "      - FFT 頻率分析\n",
    "      - RMS 計算\n",
    "    回傳 dict: {rms_x, rms_y, ratio, f1, f2, k1, k2, var1, var2, corr}\n",
    "    \"\"\"\n",
    "    nan_result = {k: np.nan for k in ['rms_x', 'rms_y', 'ratio', 'f1', 'f2', 'k1', 'k2',\n",
    "                                       'var1', 'var2', 'corr']}\n",
    "\n",
    "    x_clean = clean_data_with_outlier_filter(x_raw, allow_interpolation=False)\n",
    "    y_clean = clean_data_with_outlier_filter(y_raw, allow_interpolation=False)\n",
    "\n",
    "    if len(y_clean) == 0:\n",
    "        return nan_result, 0\n",
    "\n",
    "    x_med = medfilt(x_clean, kernel_size=5)\n",
    "    y_med = medfilt(y_clean, kernel_size=5)\n",
    "\n",
    "    # 估算 fps（由 t_a 推算）\n",
    "    dt_arr = np.diff(t_a)\n",
    "    dt_mean = np.mean(dt_arr[dt_arr > 0]) if np.any(dt_arr > 0) else 1/30\n",
    "    start_idx = auto_trim_index(y_med, skip_seconds=3, dt=dt_mean)\n",
    "\n",
    "    x = x_med[start_idx:]\n",
    "    y = y_med[start_idx:]\n",
    "\n",
    "    # --- PCA ---\n",
    "    df_pca = pd.DataFrame({'x': x, 'y': y}).dropna()\n",
    "    if len(df_pca) <= 10:\n",
    "        return nan_result, start_idx\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    pca_input_scaled = scaler.fit_transform(df_pca.values)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(pca_input_scaled)\n",
    "\n",
    "    var1, var2 = pca.explained_variance_ratio_\n",
    "    from scipy.stats import pearsonr\n",
    "    corr, _ = pearsonr(df_pca['x'], df_pca['y'])\n",
    "\n",
    "    # 繪製 PCA 圖\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc = plt.scatter(pca_result[:, 0], pca_result[:, 1],\n",
    "                     c=np.arange(len(pca_result)), cmap='viridis', s=5, alpha=0.5)\n",
    "    plt.plot(pca_result[:, 0], pca_result[:, 1], alpha=0.1, color='gray')\n",
    "    plt.axis('equal')\n",
    "    plt.title(f\"PCA [{label}]: {combined_key}\\n(PC2={var2:.1%}, corr={corr:.2f})\")\n",
    "    plt.colorbar(sc, label='Time Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- FFT 頻率分析 ---\n",
    "    dt = np.mean(np.diff(t_a[:len(pca_result)])) if len(t_a) >= len(pca_result) else dt_mean\n",
    "    f_list, k_list = [], []\n",
    "\n",
    "    for i in range(2):\n",
    "        pc_sig = pca_result[:, i] - np.mean(pca_result[:, i])\n",
    "        N = len(pc_sig)\n",
    "        yf_fft = fft(pc_sig)\n",
    "        xf = fftfreq(N, dt)[:N//2]\n",
    "        amp = 2.0/N * np.abs(yf_fft[:N//2])\n",
    "\n",
    "        mask = (xf >= 1.2) & (xf <= 5.0)\n",
    "        if np.any(mask):\n",
    "            f_mode = np.sum(xf[mask] * amp[mask]) / np.sum(amp[mask])\n",
    "            k_mode = mass * (2 * np.pi * f_mode)**2\n",
    "            f_list.append(f_mode)\n",
    "            k_list.append(k_mode)\n",
    "        else:\n",
    "            f_list.append(np.nan)\n",
    "            k_list.append(np.nan)\n",
    "\n",
    "    # --- RMS ---\n",
    "    rms_x = np.sqrt(np.mean((x - np.mean(x))**2)) if len(x) > 0 else np.nan\n",
    "    rms_y = np.sqrt(np.mean((y - np.mean(y))**2)) if len(y) > 0 else np.nan\n",
    "    ratio  = rms_y / rms_x if (rms_x and rms_x != 0) else np.nan\n",
    "\n",
    "    return {\n",
    "        'rms_x': rms_x, 'rms_y': rms_y, 'ratio': ratio,\n",
    "        'f1': f_list[0], 'f2': f_list[1],\n",
    "        'k1': k_list[0], 'k2': k_list[1],\n",
    "        'var1': var1,    'var2': var2,    'corr': corr\n",
    "    }, start_idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 主流程\n",
    "# ============================================================\n",
    "\n",
    "# 搜尋所有 xlsx\n",
    "xlsx_files = []\n",
    "for folder in target_folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        xlsx_files.extend(glob.glob(os.path.join(folder_path, \"**\", \"*.xlsx\"), recursive=True))\n",
    "\n",
    "print(f\"找到 {len(xlsx_files)} 個 xlsx 檔案\\n\")\n",
    "\n",
    "for tracker_file in xlsx_files:\n",
    "    print(\"處理:\", tracker_file)\n",
    "\n",
    "    # 提取組別\n",
    "    file_name = os.path.basename(tracker_file)\n",
    "    parts = file_name.split('-')\n",
    "    if len(parts) >= 2:\n",
    "        folder_num = parts[0]\n",
    "        group_name = parts[1]\n",
    "        combined_key = f\"{folder_num}-{group_name}\"\n",
    "    else:\n",
    "        print(\"  檔名格式不符，跳過\")\n",
    "        continue\n",
    "\n",
    "    mass = mass_dict.get(combined_key, default_mass)\n",
    "    print(f\"  組別: {combined_key}, 質量: {mass} kg\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(tracker_file)\n",
    "    except Exception as e:\n",
    "        print(f\"  讀取失敗: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ── 讀取欄位 ──────────────────────────────────────────\n",
    "    # 側面（前 8 欄）\n",
    "    t_s_raw  = df.iloc[:, 0].values   # 側面時間\n",
    "    ya_raw   = df.iloc[:, 1].values\n",
    "    yb_raw   = df.iloc[:, 2].values\n",
    "    yc_raw   = df.iloc[:, 3].values\n",
    "    yd_raw   = df.iloc[:, 4].values\n",
    "    ye_raw   = df.iloc[:, 5].values\n",
    "    yf_raw   = df.iloc[:, 6].values\n",
    "    yg_raw   = df.iloc[:, 7].values\n",
    "\n",
    "    # 側面（倒數 9~7 欄）\n",
    "    yh_raw   = df.iloc[:, -9].values\n",
    "    yi_raw   = df.iloc[:, -8].values\n",
    "    yj_raw   = df.iloc[:, -7].values\n",
    "\n",
    "    # 俯瞰（倒數 5~1 欄）\n",
    "    t_a_raw  = df.iloc[:, -5].values   # 俯瞰時間\n",
    "    xaa_raw  = df.iloc[:, -4].values\n",
    "    yaa_raw  = df.iloc[:, -3].values\n",
    "    xab_raw  = df.iloc[:, -2].values\n",
    "    yab_raw  = df.iloc[:, -1].values\n",
    "\n",
    "    # ── 1. 清理時間序列 ───────────────────────────────────\n",
    "    t_s = clean_data_with_outlier_filter(t_s_raw, time_data=True, allow_interpolation=False)\n",
    "    t_a = clean_data_with_outlier_filter(t_a_raw, time_data=True, allow_interpolation=False)\n",
    "\n",
    "    # ── 2. 俯瞰組 A (xaa, yaa) ────────────────────────────\n",
    "    print(\"  [俯瞰 A] xaa / yaa\")\n",
    "    res_a, start_idx_a = process_overhead_pair(\n",
    "        xaa_raw, yaa_raw, t_a, mass, label=\"A(xaa,yaa)\", combined_key=combined_key\n",
    "    )\n",
    "\n",
    "    # ── 3. 俯瞰組 B (xab, yab) ────────────────────────────\n",
    "    print(\"  [俯瞰 B] xab / yab\")\n",
    "    res_b, start_idx_b = process_overhead_pair(\n",
    "        xab_raw, yab_raw, t_a, mass, label=\"B(xab,yab)\", combined_key=combined_key\n",
    "    )\n",
    "\n",
    "    # 取兩組俯瞰平均（或視需求改成各自記錄）\n",
    "    rms_x  = np.nanmean([res_a['rms_x'], res_b['rms_x']])\n",
    "    rms_y  = np.nanmean([res_a['rms_y'], res_b['rms_y']])\n",
    "    ratio  = np.nanmean([res_a['ratio'],  res_b['ratio']])\n",
    "\n",
    "    # ── 4. 側面數據：ya~yg + yh~yj 共 10 個測量點 ─────────\n",
    "    # 使用 start_idx_a 作為同步裁切點（可改為 start_idx_b 或兩者取平均）\n",
    "    side_raw_dict = {\n",
    "        \"ya\": ya_raw, \"yb\": yb_raw, \"yc\": yc_raw,\n",
    "        \"yd\": yd_raw, \"ye\": ye_raw, \"yf\": yf_raw,\n",
    "        \"yg\": yg_raw, \"yh\": yh_raw, \"yi\": yi_raw, \"yj\": yj_raw\n",
    "    }\n",
    "    layers = {}\n",
    "\n",
    "    for key, raw_val in side_raw_dict.items():\n",
    "        side_clean = clean_side_data(raw_val, t_s_raw)\n",
    "        if len(side_clean) > 0:\n",
    "            side_med = medfilt(side_clean, kernel_size=5)\n",
    "            # 與俯瞰數據同步裁切\n",
    "            layers[key] = side_med[start_idx_a:] if len(side_med) > start_idx_a else side_med\n",
    "        else:\n",
    "            layers[key] = np.array([])\n",
    "\n",
    "    # FFT 頻率分析（側面 10 點）\n",
    "    main_freqs = []\n",
    "    for key, y_layer in layers.items():\n",
    "        if len(y_layer) < 10 or len(t_s) < 10:\n",
    "            continue\n",
    "\n",
    "        y_centered = y_layer - np.mean(y_layer)\n",
    "        dt = np.mean(np.diff(t_s[:len(y_centered)]))\n",
    "        if dt <= 0:\n",
    "            continue\n",
    "\n",
    "        N = len(y_centered)\n",
    "        yf_fft = fft(y_centered)\n",
    "        xf     = fftfreq(N, dt)[:N//2]\n",
    "        amplitude = 2.0/N * np.abs(yf_fft[:N//2])\n",
    "\n",
    "        mask = (xf >= 1.2) & (xf <= 5.0)\n",
    "        if np.any(mask):\n",
    "            main_freq = np.sum(xf[mask] * amplitude[mask]) / np.sum(amplitude[mask])\n",
    "            print(f\"    {key} 主頻率: {main_freq:.2f} Hz\")\n",
    "            main_freqs.append(main_freq)\n",
    "\n",
    "    if len(main_freqs) > 0:\n",
    "        f_n = np.mean(main_freqs)\n",
    "        print(f\"  側面系統自然頻率: {f_n:.2f} Hz\")\n",
    "    else:\n",
    "        f_n = np.nan\n",
    "        print(\"  側面無法計算頻率\")\n",
    "\n",
    "    k = mass * (2 * np.pi * f_n)**2 if not np.isnan(f_n) else np.nan\n",
    "\n",
    "    # ── 5. 加速度 (ZIP → CSV) ─────────────────────────────\n",
    "    folder_dir = os.path.dirname(tracker_file)\n",
    "    base_name  = os.path.splitext(os.path.basename(tracker_file))[0]\n",
    "    all_zips   = glob.glob(os.path.join(folder_dir, \"*.zip\"))\n",
    "    zip_file   = next((z for z in all_zips if\n",
    "                       os.path.splitext(os.path.basename(z))[0] == base_name), None)\n",
    "\n",
    "    rms_acc = np.nan\n",
    "    if zip_file:\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "                csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
    "                if csv_files:\n",
    "                    with z.open(csv_files[0]) as f:\n",
    "                        acc_df = pd.read_csv(f)\n",
    "                        acc_abs_raw = acc_df.iloc[:, -1].values\n",
    "                        acc_series  = pd.to_numeric(pd.Series(acc_abs_raw),\n",
    "                                                    errors='coerce').dropna().values\n",
    "                        if len(acc_series) > 0:\n",
    "                            acc_filtered = acc_series[filter_outliers(acc_series, threshold=0.1)]\n",
    "                            rms_acc = (np.sqrt(np.mean(acc_filtered**2))\n",
    "                                       if len(acc_filtered) > 0 else np.nan)\n",
    "        except Exception as e:\n",
    "            print(f\"  讀取 zip 錯誤: {e}\")\n",
    "\n",
    "    # ── 6. 儲存結果 ───────────────────────────────────────\n",
    "    results.append([\n",
    "        tracker_file, combined_key,\n",
    "        rms_x, rms_y, ratio,\n",
    "        f_n, k,\n",
    "        res_a['f1'], res_a['f2'],      # 俯瞰 A 的 PC1/PC2 頻率\n",
    "        res_b['f1'], res_b['f2'],      # 俯瞰 B 的 PC1/PC2 頻率\n",
    "        rms_acc\n",
    "    ])\n",
    "\n",
    "    if combined_key not in group_data:\n",
    "        group_data[combined_key] = {p: [] for p in\n",
    "            ['rms_x', 'rms_y', 'ratio', 'f_n', 'k',\n",
    "             'fa1', 'fa2', 'fb1', 'fb2', 'rms_acc']}\n",
    "\n",
    "    for field, val in zip(\n",
    "        ['rms_x', 'rms_y', 'ratio', 'f_n', 'k', 'fa1', 'fa2', 'fb1', 'fb2', 'rms_acc'],\n",
    "        [rms_x,  rms_y,  ratio,  f_n,  k,\n",
    "         res_a['f1'], res_a['f2'], res_b['f1'], res_b['f2'], rms_acc]\n",
    "    ):\n",
    "        group_data[combined_key][field].append(val)\n",
    "\n",
    "    print(f\"  ✓ 已處理 {combined_key}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 結果彙整\n",
    "# ============================================================\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    \"File\", \"Group\",\n",
    "    \"RMS_x\", \"RMS_y\", \"偏心比例\",\n",
    "    \"側面主頻率(Hz)\", \"側面等效剛性(N/m)\",\n",
    "    \"俯瞰A_PC1頻率(Hz)\", \"俯瞰A_PC2頻率(Hz)\",\n",
    "    \"俯瞰B_PC1頻率(Hz)\", \"俯瞰B_PC2頻率(Hz)\",\n",
    "    \"RMS加速度\"\n",
    "])\n",
    "\n",
    "# 不確定度計算\n",
    "uncertainty_results = []\n",
    "\n",
    "for grp_name, data in group_data.items():\n",
    "    print(f\"\\n計算 {grp_name} 的不確定度:\")\n",
    "    uncertainties = {}\n",
    "\n",
    "    for param_name, values in data.items():\n",
    "        valid_values = [v for v in values if not np.isnan(v)]\n",
    "        n = len(valid_values)\n",
    "\n",
    "        if n >= 2:\n",
    "            mean_val = np.mean(valid_values)\n",
    "            std_val  = np.std(valid_values, ddof=1)\n",
    "            u_val    = std_val / np.sqrt(n)\n",
    "            rel_u    = (u_val / mean_val * 100) if mean_val != 0 else np.nan\n",
    "\n",
    "            uncertainties[param_name] = {'mean': mean_val, 'std': std_val,\n",
    "                                          'u': u_val, 'relative_u': rel_u, 'n': n}\n",
    "            print(f\"  {param_name}: 均值={mean_val:.4f}, u={u_val:.4f}, \"\n",
    "                  f\"相對不確定度={rel_u:.2f}% (n={n})\")\n",
    "        else:\n",
    "            uncertainties[param_name] = {\n",
    "                'mean': valid_values[0] if n == 1 else np.nan,\n",
    "                'std': np.nan, 'u': np.nan, 'relative_u': np.nan, 'n': n\n",
    "            }\n",
    "            print(f\"  {param_name}: 樣本數不足 (n={n})\")\n",
    "\n",
    "    row = {'Group': grp_name}\n",
    "    for p in ['rms_x', 'rms_y', 'ratio', 'f_n', 'k', 'fa1', 'fa2', 'fb1', 'fb2', 'rms_acc']:\n",
    "        row[f'{p}_mean'] = uncertainties[p]['mean']\n",
    "        row[f'{p}_u']    = uncertainties[p]['u']\n",
    "    row['sample_size'] = uncertainties['rms_x']['n']\n",
    "    uncertainty_results.append(row)\n",
    "\n",
    "uncertainty_df = pd.DataFrame(uncertainty_results)\n",
    "\n",
    "# 輸出\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"分析結果彙總:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"各組不確定度統計:\")\n",
    "print(\"=\"*80)\n",
    "print(uncertainty_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(\"analysis_results.csv\", index=False, encoding='utf-8-sig')\n",
    "uncertainty_df.to_csv(\"uncertainty.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n結果已儲存至:\")\n",
    "print(\"  - analysis_results.csv（詳細結果）\")\n",
    "print(\"  - uncertainty.csv（不確定度統計）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4371a",
   "metadata": {},
   "source": [
    "Above code were provided by Claude and fixed by human."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
